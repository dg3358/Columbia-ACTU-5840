# Homework 1 - Predictive Modeling in Finance and Insurance

# By: Dennis Goldenberg

```{r echo, include = TRUE}
# import packages
library(ggplot2)
library(patchwork)
library(MASS)
library(magrittr)
```

## 1. Automobile Insurance Claims

```{r, include = TRUE}
# read in Data
auto <- read.table(file = 'AutoClaims-1.csv', header = TRUE, sep = ',')
auto$state <- factor(auto$state, ordered = TRUE)
auto$gender <- factor(auto$gender)
auto$class <- factor(auto$class)
```

### 1a.

I compute the descriptive statistics for the "PAID" variable:

```{r, inlcude = TRUE}
summary(auto$paid)
```

So, the mean paid is \$1,853.00 and the median paid is \$1,001.70.

### 1b.

I graph the histogram and qqplot of "paid", comparing quantiles:

```{r, include = TRUE}
hist <- ggplot(data = auto) + 
        geom_histogram(aes(paid), binwidth = 1000) +
        labs(title = "Histogram of Payouts") + 
        theme(plot.title = element_text(hjust = 0.5))
qq <- ggplot(data = auto) + geom_qq(aes(sample = paid)) +
      geom_qq_line(aes(sample = paid)) +
      labs(x = "Standard Normal Quantiles", y = 'Quantiles, Paid') + 
      labs(title = 'QQ plot') + 
      theme(plot.title = element_text(hjust = 0.5))
hist + plot_spacer() + qq + plot_layout(widths = c(6,0.5,6))
```

The qqplot suggests that the normal distribution is not a good fit. Based on the histogram, the distribution of paid looks right-skewed, and potentially $\textbf{lognormal}$.

### 1c.

I graph the histogram and qqplot of "logpaid", comparing quantiles:

```{r, include = TRUE}
hist <- ggplot(data = auto) + 
        geom_histogram(aes(logpaid), binwidth = 0.25) +
        labs(title = "Histogram of Payouts") + 
        theme(plot.title = element_text(hjust = 0.5))
qq <- ggplot(data = auto) + geom_qq(aes(sample = logpaid)) +
      geom_qq_line(aes(sample = logpaid)) +
    labs(x = "Standard Normal Quantiles", y = 'Quantiles, logPaid') +
      labs(title = 'QQ plot') + 
      theme(plot.title = element_text(hjust = 0.5))
hist + plot_spacer() + qq + plot_layout(widths = c(6,0.5,6))
```

Based on the good fit in the qqplot, and the general shape in the histogram, the distribution of "logpaid" seems to be $\textbf{approximately normal}$. This would suggest that the distribution of paid is $\textbf{lognormal}$.

### 1d.

I graph the scatterplot of "logpaid" against "age":

```{r, inlcude = TRUE}
scat <- ggplot(data = auto) + geom_point(aes(x = age, y = logpaid)) +
        labs(title = "Natural log of Paid Amount vs. age") +
        theme(plot.title = element_text(hjust = 0.5))
scat
```

This plot suggests that the variance of log payments for individuals on the younger side (closer to 50) is higher than the variance of payments for older individuals; outside of this, however, there appears to be $\textbf{no discernable relationship}$ between the natural log of the paid amount for an automobile insurance claim and the age of an individual.

### 1e.

I account for gender in the next scatter plot:

```{r, inlcude = TRUE}
scat <- ggplot(data = auto) + 
        geom_point(aes(x = age, y = logpaid, color = gender)) +
        labs(title = "Natural log of Paid Amount vs. age") +
        theme(plot.title = element_text(hjust = 0.5))
scat
```

Based on the scatter plot above, there is $\textbf{no discernable relationship}$ between the natural log of the paid amount for an automobile insurance claim and the gender of an individual.

## 2. Boston Housing Dataset

### 2a.

I first show to pairwise scatter plots of all combinations of variables:

```{r, include = TRUE}
pairs(Boston, col = 'blue')
```

This output is too small to discern any patterns; I look at the correlation matrix to see which scatter plots may have a strong relationship. I find where the correlations are strongest by generating a correlation matrix and breaking it into two to see all columns:

```{r, include = TRUE}
cormatrix <- cor(Boston)
twocor <- matrix(as.numeric(sprintf(cormatrix, fmt = '%#.2f')), nrow = dim(cormatrix)[1])
rownames(twocor) <- rownames(cormatrix)
colnames(twocor) <- colnames(cormatrix)
twocor[,1:7]
```

```{r, include = TRUE}
twocor[,8:14]
```

I pick the 6 pairs with the highest correlation to plot:

```{r, include = TRUE}
radtax <- ggplot(Boston) + geom_point(aes(x = rad, y = tax))
noxdis <- ggplot(Boston) + geom_point(aes(x = nox, y = dis))
noxindus <- ggplot(Boston) + geom_point(aes(x = nox, y = indus))
agedis <- ggplot(Boston) + geom_point(aes(x = age, y = dis))
lstatmedv <- ggplot(Boston) + geom_point(aes(x = lstat, y = medv))
noxage <- ggplot(Boston) + geom_point(aes(x = nox, y = age))

(radtax+plot_spacer()+noxdis+plot_layout(widths = c(6,0.5,6)))/
  plot_spacer()/
  (noxindus+plot_spacer()+agedis+plot_layout(widths = c(6,0.5,6)))/
  plot_spacer()/
  (lstatmedv+plot_spacer()+noxage+plot_layout(widths = c(6,0.5,6))) +
  plot_layout(heights = c(6, 0.5, 6, 0.5, 6))
```

Here, it is notable that "rad", or accessibility to highways, is $\textbf{directly related}$ with "tax", or property tax rate per \$10,000. Further, "nox", or nitric oxide concentration, is $\textbf{inversely related}$ with "dis", or distance to employment centers with the relationship looking like exponential decay, but $\textbf{directly related}$ with "indus", or proportion of non-retail business acres, and "age", or proportion of buildings built before 1940. Finally, "dis" is $\textbf{inversely related}$ with "age", and "medv", or the median value of owner occupied homes, is $\textbf{inversely related}$ to "lstat", or percent of the population in lower status.

### 2b.

I check the correlation matrix with regards to crime rate:

```{r, include = TRUE}
twocor[1:7,'crim']
twocor[8:14,'crim']
```

I pick the 6 variables with the strongest correlation (excluding nox, to be evaluated later):

```{r, include = TRUE}
crimrad <- ggplot(Boston) + geom_point(aes(x = rad, y = crim))
crimtax <- ggplot(Boston) + geom_point(aes(x = tax, y = crim))
crimlstat <- ggplot(Boston) + geom_point(aes(x = lstat, y = crim))
crimindus <- ggplot(Boston) + geom_point(aes(x = indus, y = crim))
crimblack <- ggplot(Boston) + geom_point(aes(x = black, y = crim))
crimmedv <- ggplot(Boston) + geom_point(aes(x = medv, y = crim))

(crimrad+plot_spacer()+crimtax+plot_layout(widths = c(6,0.5,6)))/
  plot_spacer()/
(crimlstat+plot_spacer()+crimindus+plot_layout(widths = c(6,0.5,6)))/
  plot_spacer()/
(crimblack+plot_spacer()+crimmedv+plot_layout(widths = c(6,0.5,6))) +
  plot_layout(heights = c(6, 0.5, 6, 0.5, 6))
```

The strongest relationships with per capital crime rate, according to these graphs, are 'lstat", 'black', and 'medv'. It seems as though there is a $\textbf{direct relationship}$ between 'lstat' and the crime rate; that is, the higher the proportion of a population is of lower status, the higher there are crime rates. Meanwhile, though these relationships appear weaker, there is an $\textbf{inverse relationship}$ between crime rates and the proportion of population that is black and the median value of owner occupied homes. I next graph some of the other variables with relation to crim to see if there is a relationship:

```{r, include = TRUE}
crimnox <- ggplot(data = Boston) + geom_point(aes(x = nox, y = crim))
crimdis <- ggplot(data = Boston) + geom_point(aes(x = dis, y = crim))
crimage <- ggplot(data = Boston) + geom_point(aes(x = age, y = crim))
crimr <- ggplot(data = Boston) + geom_point(aes(ptratio,crim))
((crimnox+plot_spacer()+crimdis+plot_layout(widths = c(6,0.5,6)))/
  plot_spacer()/
  (crimage+plot_spacer()+crimr+plot_layout(widths = c(6,0.5,6))))+
  plot_layout(heights = c(6, 0.5, 6))
```

Here, it appears as though crime rate has a $\textbf{direct relationship}$ with nitric oxide concentration; the areas with the highest crime rates tend to have higher crime rates; it is also true that the areas with the highest crime rates tend to have higher proportions of buildings built before 1940. Alternatively, the crime rate seems to be $\textbf{inversely related}$ to distance to employment centers; the areas closest to the employment centers tend to have the highest crime rates.

### 2c.

The maximum, minimum, mean, and range of each variable is listed below:

```{r, include = TRUE}
max_min <- cbind(apply(Boston, 2, min), apply(Boston, 2, mean),
                 apply(Boston, 2, max))
max_min2 <- cbind(max_min, max_min[,3] - max_min[,1])
colnames(max_min2) <- c("Min", "Mean", "Max", "Range")
max_min2
```

The most notable results from this are the following: there is a wide range of tax rates and crime rates, because there is no theoretical ceiling as to how high they can go. Meanwhile, nitric oxide concentration has a much smaller range, but smaller movements in this predictor could be potentially more important due to the environmental changes caused. The black variable is a formula with a squared term, contributing to its large range. However, the age, zn, and indus variables are all proportions, so their theoretical range can only go from 0 to 100. Finally, though it seems that the median value of homes, or medv, has a smaller range than other variables, that it is because it was recorded in terms of \$1000s; in reality, it actually has the highest range, even higher than tax rates or crime rates. Next, I calculate the number of suburbs with one of the following: a crime rate per capita above 60, a tax rate per \$10,000 above 600, or a pupil-teacher ratio above 20:

```{r, include = TRUE}
hc <- as.numeric(Boston['crim'] > 60) %>% sum
ht <- as.numeric(Boston['tax'] > 600) %>% sum
hpt <- as.numeric(Boston['ptratio'] > 20) %>% sum
highs <- as.matrix(c(hc, ht, hpt))
rownames(highs) <- c("High Crime","High Tax", "High PT")
highs
```

There are $\mathbf{3}$ suburbs with a crime rate above 60, $\mathbf{137}$ with a tax rate about 600, and $\mathbf{201}$ with a pupil-teacher ratio above 20.

### 2d.

The variable representing whether or not a suburb bounds the Charles river is 'chas'; it is a simple binary variable, with 1 indicating that the suburb bounds the river. I check how many satisfy this in total by summing the column:

```{r, include = TRUE}
Boston[['chas']] %>% sum
```

So, $\mathbf{35}$ suburbs bound the Charles river.

### 2e.

I check the summary of the ptratio:

```{r, include = TRUE}
summary(Boston['ptratio'])
```

The median pupil-teacher ratio is $\textbf{19.05}$ students per teacher.

### 2f.

```{r, include = TRUE}
index <- which.min(Boston[['medv']])
as.vector(Boston[index,])
```

The 399th suburb (value of index) has the lowest median value of owner-occupied homes, at \$5,000. It achieves maximum possible values for the "age" (100% of homes in this suburb were built before 1940), black, and "rad" (accessibility to highways) variables. It has an above average nitric oxide concentration, tax rate, pupil-teacher ratio, non-retail business acre use rate ('indus'), weighted distance to employment centers ('dis'), and percent of inhabitants in lower status ('lstat'). It is below average for number of rooms per dwelling, and has no residential area zoned for lots above 25,000 square feet. It seems like a difficult area to live in.

### 2g.

I calculate the desired quantity by looking at the amounts asked about:

```{r, include = TRUE}
g7 <- Boston[Boston['rm'] > 7,]
g8 <- Boston[Boston['rm'] > 8,]
counts <- c(dim(g7)[1], dim(g8)[1])
counts
```

There are $\mathbf{64}$ suburbs that average more than seven rooms per dwelling, and $\mathbf{13}$ that average more than eight rooms per dwelling. I then examine some of the characteristics of these subsets of the data. I first examine the minimum, mean, max, and range for those with more than seven rooms per dwelling:

```{r, include = TRUE}
mmg7 <- cbind(apply(g7, 2, min), apply(g7, 2, mean),
                 apply(g7, 2, max))
mmg72 <- cbind(mmg7, mmg7[,3] - mmg7[,1])
colnames(mmg72) <- c("Min", "Mean", "Max", "Range")
mmg72
```

On average, these suburbs contain lower amounts of the following: crime (and none of the highest crime suburbs have more than 7 rooms per dwelling), proportion of the land set aside for non-residential business, proportion of buildings built pre 1940, pupil-teacher ratio, proportion of lower status people, nitric oxide concentration, accessibility to highways, and tax rate. On the other hand, these suburbs are higher in the following: proportion of land set aside for \>25,000 acre plots, likelihood of bounding the Charles River, variable associated with proportion of the population that is black, and median value of owner occupied homes. Most of these results are in line with more valuable properties being in said suburbs, as the average dwelling is bigger than average among all suburbs. I repeat this for suburbs that average more than 8 rooms per dwelling:

```{r, include = TRUE}
mmg8 <- cbind(apply(g8, 2, min), apply(g8, 2, mean),
                 apply(g8, 2, max))
mmg82 <- cbind(mmg8, mmg8[,3] - mmg8[,1])
colnames(mmg82) <- c("Min", "Mean", "Max", "Range")
mmg82
```

Among these suburbs, certain predictors vary in the same way but to a more extreme amount than those that average more than 7 rooms: on average, crime is even lower, the likelihood of bounding the Charles river is higher, distance to employment centers is even lower, the proportion of lower status people is even lower. Certain predictors vary in the same way but to a less extreme amount: average proportion of land for \>25,000 acre plots, proportion of land, tax rate, the feature corresponding to the proportion of black population, and the proportion of non-retail business acres is greater than global average but not by as much. Similarly, average nitric oxide concentration, pupil-teacher ratio, and accessibility to highways is lower than global average but not by as much. The remaining variables reversed polarity: average distance to employment centers is lower instead of higher and average proportion of pre-1940 building is higher instead of lower.

## 3. KNearestNeighbors

```{r, include = TRUE}
# create data set
x_1 <- c(0,2,0,0,-1,1,-1)
x_2 <- c(3,0,1,1,0,1,2)
x_3 <- c(0,1,3,2,1,1,-1)
y <- c('Red','Red','Red','Green','Green','Red','Green')
KNN_data <- as.data.frame(cbind(x_1,x_2,x_3,y))
```

### 3a.

To calculate the euclidean distance between each point and $(0,0,0)$, I simply need the square root of the sum of the squared features of each individual observation:

```{r, include = TRUE}
obs_data <- KNN_data[c('x_1','x_2','x_3')]
vec_data <- obs_data %>% as.matrix %>% as.numeric
mdata <- vec_data %>% as.vector
dim(mdata) <- c(7,3)
colnames(mdata) <- c('x_1','x_2','x_3')
sqdist <- mdata^2 %>% rowSums
dist <- sqdist^(0.5)
distdata <- as.data.frame(cbind(1:7,dist))
colnames(distdata) <- c('Obs','dist')
distdata
```

### 3b.

The closest observation by Euclidean distance is observation 5. Observation 5 has the label 'Green', so if K = 1, the prediction would be $\textbf{Green}$.

### 3c.

The closest two observations are 5 and 6. Observation 5 is labeled 'Green' and 6 is labeled 'Red'; however, there is a tie for the next closest between observations 2 and 4, which have opposing labels. If the tiebreaker goes to observation 2, then there are two 'Red' labels and one 'Green' label closest to the origin, causing the prediction to be 'Red', but if observation 4 wins the tiebreaker, the prediction would be 'Green'. Consequently, if K = 3, the origin point $\textbf{exists on the decision boundary}$.

### 3d.

We would expect the best value (a.k.a the closest distance)
