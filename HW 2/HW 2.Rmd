---
title: "HW 2 - Predictive Modeling in Finance and Insurance"
author: "Dennis Goldenberg"
date: "2024-02-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Nursing Home Utilization

```{r echo, include = TRUE}
# import packages
library(ggplot2)
library(magrittr)
```

```{r, include = TRUE}
# read in data
# WNH <- read.csv(file)
WNH <- read.csv('WiscNursingHome.csv', header = TRUE)
WNH$CRYEAR <- factor(WNH$CRYEAR)
WNH <- WNH[WNH$CRYEAR == 2001,]
```


## 1a) Estimation of Coefficients

```{r, include = TRUE}
#Generate variables to analyze
WNH$LOGTPY <- log(WNH$TPY)
WNH$LOGNUMBED <- log(WNH$NUMBED)
```

Using the generated variables, I calculate $x^Tx$, adding in a column for the intercept:
```{r, include = TRUE}
x <- cbind(1,WNH$LOGNUMBED)
xTx <- t(x) %*% x
xTx
```

Then, I find $\left(x^Tx\right)^{-1}$:

```{r, include = TRUE}
xTxInv <- solve(xTx)
xTxInv
```

Finally, I find $x^Ty$:
```{r, include = TRUE}
y <- WNH$LOGTPY
xTy <- t(x) %*% y
xTy
```

Using the formula for linear regression that $\beta = (x^Tx)^{-1}x^Ty$:
```{r, include = TRUE}
beta <- xTxInv %*% xTy
beta
```


## 1b. The prediction Matrix
Since $\hat{y} = x\hat{\beta}$, and $\beta = (x^Tx)^{-1}x^Ty$, the prediction matrix $H = x\left(x^Tx\right)^{-1}x^T$, so:
\begin{equation*}
\hat{y} = x(x^Tx)^{-1}x^Ty = Hy
\end{equation*}
I find the diagonals of said matrix $H$ and store them in "leverages" variable, as they represent the leverage of each data point; the first 6 outputs are shown below to verify with the Excel document:

```{r, include = TRUE}
H <- x %*% xTxInv %*% t(x)
leverages <- diag(H)
head(leverages)
```

## 1c. Making Predictions

Since $\hat{y} = Hy$, I calculate and store in the "pred" variable, showing the first 6 predicted values for verification with excel:
```{r, include = TRUE}
pred <- H %*% y 
head(pred)
```

## 1d. Calculating Summary Statistics
The $R^2$ value is the proportion of variation explained by the regression. $R^2_{adj}$ is adjusted for the number of predictors; its formula is:
\begin{equation*}
R^2_{adj} = 1 - \frac{\frac{SSE}{n - p - 1}}{\frac{SST}{n - 1}} = 1 - \frac{\frac{SSE}{n - 2}}{\frac{SST}{n - 1}}
\end{equation*}
Then, the F statistic measures the significance of the regression; its formula is:
\begin{equation*}
F_{stat} = \frac{\frac{SST - SSE}{p}}{\frac{SSE}{N - (p + 1)}} = \frac{SST - SSE}{\frac{SSE}{N - 2}}
\end{equation*}
The $p$-value is simply the probability that so much variation was observed by a model with no predictive power:
\begin{equation*}
p = \mathbb{P}(F \geq F_{stat}), \text{ where } F \sim \text{F-dist}(1,N - 2)
\end{equation*}
Finally, the mean squared error is just the sum of squared error divided by the number of degrees of freedom for said error, or $\frac{SSE}{N - 2}$. All are calculated below:

```{r, include = TRUE}
SSR <- sum((mean(WNH$LOGTPY) - pred)^2)
SSE <- sum((WNH$LOGTPY - pred)^2)
SST <- sum((WNH$LOGTPY - mean(WNH$LOGTPY))^2)
```
