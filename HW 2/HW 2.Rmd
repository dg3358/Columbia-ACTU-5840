---
title: "HW 2 - Predictive Modeling in Finance and Insurance"
author: "Dennis Goldenberg"
date: "2024-02-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Nursing Home Utilization

```{r echo, include = TRUE}
# import packages
library(ggplot2)
library(magrittr)
```

```{r, include = TRUE}
# read in data
# WNH <- read.csv(file)
WNH <- read.csv('WiscNursingHome.csv', header = TRUE)
WNH$CRYEAR <- factor(WNH$CRYEAR)
WNH <- WNH[WNH$CRYEAR == 2001,]
```


## 1a) Estimation of Coefficients

```{r, include = TRUE}
#Generate variables to analyze
WNH$LOGTPY <- log(WNH$TPY)
WNH$LOGNUMBED <- log(WNH$NUMBED)
```

Using the generated variables, I calculate $x^Tx$, adding in a column for the intercept:
```{r, include = TRUE}
x <- cbind(1,WNH$LOGNUMBED)
xTx <- t(x) %*% x
xTx
```

Then, I find $\left(x^Tx\right)^{-1}$:

```{r, include = TRUE}
xTxInv <- solve(xTx)
xTxInv
```

Finally, I find $x^Ty$:
```{r, include = TRUE}
y <- WNH$LOGTPY
xTy <- t(x) %*% y
xTy
```

Using the formula for linear regression that $\beta = (x^Tx)^{-1}x^Ty$:
```{r, include = TRUE}
beta <- xTxInv %*% xTy
beta
```


## 1b. The prediction Matrix
Since $\hat{y} = x\hat{\beta}$, and $\beta = (x^Tx)^{-1}x^Ty$, the prediction matrix $H = x\left(x^Tx\right)^{-1}x^T$, so:
\begin{equation*}
\hat{y} = x(x^Tx)^{-1}x^Ty = Hy
\end{equation*}
I find the diagonals of said matrix $H$ and store them in "leverages" variable, as they represent the leverage of each data point; the first 6 outputs are shown below to verify with the Excel document:

```{r, include = TRUE}
H <- x %*% xTxInv %*% t(x)
leverages <- diag(H)
head(leverages)
```

## 1c. Making Predictions

Since $\hat{y} = Hy$, I calculate and store in the "pred" variable, showing the first 6 predicted values for verification with excel:
```{r, include = TRUE}
pred <- H %*% y 
head(pred)
```

## 1d. Calculating Summary Statistics
The $R^2$ value is the proportion of variation explained by the regression. $R^2_{adj}$ is adjusted for the number of predictors; its formula is:
\begin{equation*}
R^2_{adj} = 1 - \frac{\frac{SSE}{n - p - 1}}{\frac{SST}{n - 1}} = 1 - \frac{\frac{SSE}{n - 2}}{\frac{SST}{n - 1}}
\end{equation*}
Then, the F statistic measures the significance of the regression; its formula is:
\begin{equation*}
F_{stat} = \frac{\frac{SST - SSE}{p}}{\frac{SSE}{N - (p + 1)}} = \frac{SST - SSE}{\frac{SSE}{N - 2}}
\end{equation*}
The $p$-value is simply the probability that so much variation was observed by a model with no predictive power:
\begin{equation*}
p = \mathbb{P}(F \geq F_{stat}), \text{ where } F \sim \text{F-dist}(1,N - 2)
\end{equation*}
Finally, the mean squared error is just the sum of squared error divided by the number of degrees of freedom for said error, or $\frac{SSE}{N - 2}$. All are calculated below:

```{r, include = TRUE}
SSR <- sum((mean(WNH$LOGTPY) - pred)^2)
SSE <- sum((WNH$LOGTPY - pred)^2)
SST <- sum((WNH$LOGTPY - mean(WNH$LOGTPY))^2)
n <- length(WNH$CRYEAR)
R_2 <- SSR/SST
R_2_adj <- 1 - (SSE/(n - 2))/(SST/(n - 1))
F_stat <- (SST - SSE)/(SSE/(n - 2))
p_reg <- 1 - pf(F_stat, 1, n - 2)
MSE <- SSE/(n - 2)
sumStats <- c(R_2, R_2_adj, F_stat, p_reg, MSE)
names(sumStats) <- c("R^2", "adj. R^2", "F", "p-val", "MSE")
t(sumStats)
```

## 1e. Calculating Residuals

For observation i, the residual and standard residual have the following formulas:
\begin{equation*}
e_i = y - y_i \text{ and } e_{i,std.} = \frac{e_i}{\sqrt{MSE}*\sqrt{1 - h_{ii}}}
\end{equation*}

I calculate both and print out the first 6 for both:
```{r, include = TRUE}
resid <- WNH$LOGTPY - pred
resid_std <- resid/(sqrt(MSE) * sqrt(1 - leverages))
f6res <- cbind(head(resid), head(resid_std))
colnames(f6res) <- c("Residuals", " Std. Residuals")
f6res
```


## 1f. Hypothesis testing
The hypotheses to test are:
\begin{equation*}
H_0: \beta_1 = 0 \text{ vs. } H_1: \beta_1 \neq 0
\end{equation*}
I calculate the t-statistic, or $t = \frac{\hat{\beta}_1}{se\left(\hat{\beta}_1\right)}$, noting that $se\left(\hat{\beta}_1\right) = \sqrt{MSE}*\sqrt{\left(x^Tx\right)^{-1}_{(2,2)}}$:
```{r, include = TRUE}
se_beta <- sqrt(MSE) * sqrt(xTxInv[2,2])
t_stat <- beta[2]/se_beta
t_stat
```

Note that $t_{stat} \sim F(ndf = 1, ddf = n - 2)$; therefore, I calculate the p-val of this statistic:
```{r, include = TRUE}
p_beta1 <- 1 - pf(t_stat, 1, n - 2)
p_beta1
```

Note that $p < 0.05$; thus, the natural log of the number of beds has a statistically significant impact on the natural log of the number of patient years, and we reject $H_0$. Next, the following formula gives the confidence interval for 95\% and 99\% (so $\alpha = 0.05$ and $\alpha = 0.01$ respectively)
\begin{equation*}
CI = \left(\hat{\beta}_1 - z^{(1 - \frac{\alpha}{2})}se\left(\hat{\beta}_1\right), \hat{\beta}_1 + z^{1 + \frac{\alpha}{2}}se\left(\hat{\beta}_1\right)\right)
\end{equation*}
I code this up and generate it form both significance levels:
```{r, include = TRUE}
lCI05 <- beta[2] - qnorm(0.975, 0, 1)*se_beta
rCI05 <- beta[2] + qnorm(0.975, 0, 1)*se_beta
lCI01 <- beta[2] - qnorm(0.995, 0, 1)*se_beta
rCI01 <- beta[2] + qnorm(0.995, 0, 1)*se_beta
CIMatrix <- matrix(data = c(lCI05,lCI01,rCI05,rCI01), nrow = 2)
rownames(CIMatrix) <- c("95% conf.", "99% conf.")
colnames(CIMatrix) <- c("Lower CI", "Upper CI")
CIMatrix
```

## 1g. Prediction
The prediction is $\widehat{\ln(y)} = \hat{\beta_0} + \hat{\beta_1}\ln(x^*)$; the prediction interval is:
\begin{equation*}
PI = \left(\hat{y} - z^{(1 - \frac{\alpha}{2})}\sqrt{MSE}\sqrt{1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n - 1)S_X^2}}, \hat{y} + z^{(1 - \frac{\alpha}{2})}\sqrt{MSE}\sqrt{1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n - 1)S_X^2}}\right)
\end{equation*}

I first show the prediction:

```{r, include = TRUE}
pred100 <- beta[1] + beta[2]*log(100)
num <- (log(100) - mean(WNH$LOGTPY))^2
denom <- sum((WNH$LOGTPY - mean(WNH$LOGTPY))^2)
lpred <- pred100 - qnorm(0.975, 0, 1)*sqrt(MSE)*sqrt(1 + 1/n + num/denom)
upred <- pred100 + qnorm(0.975, 0, 1)*sqrt(MSE)*sqrt(1 + 1/n + num/denom)
PIMatrix <- matrix(data = c(lpred, upred), nrow = 1)
colnames(PIMatrix) <- c("Lower PI", "Upper PI")
pred100
```

And now the prediction interval:
```{r, include = TRUE}
PIMatrix
```
## 1h. Applying Linear Model to check
I apply a linear model to check all of my results:
```{r, include = TRUE}
model <- lm("LOGTPY ~ LOGNUMBED", data = WNH)
summary(model)
```

My results match with the model.