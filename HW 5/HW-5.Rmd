---
title: "Homework 5 - Predictive Modeling in Finance and Insurance"
author: "Dennis Goldenberg"
date: "2024-02-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE}
library(MASS)
library(ggplot2)
library(ISLR)
```

#1. Cross Validation

## 1a. Compute LOOCV
I use the method prescribed in the homework, noting that the default for the cross validation method is Leave-One-Out Cross validation and show the result:
```{r, include = TRUE}
LOOCV <- c()
for (i in 1:10){
  polyLCV <- glm(sprintf('medv ~ poly(lstat, %f)', i), data = Boston)
  LOOCV <- append(LOOCV,boot::cv.glm(data = Boston,glmfit = polyLCV)$delta[2])
}
LCVFrame <- data.frame(cbind(1:10, LOOCV))
colnames(LCVFrame) <- c("Degree", "LOOCV")
LCVFrame
```

I graph the LOOCV plot:
```{r, include = TRUE}
ggplot(data = LCVFrame) + geom_line(aes(x = Degree,y = LOOCV)) +
  geom_point(aes(x = Degree, y = LOOCV), color = 'red') + 
  scale_x_continuous(breaks = seq(1,10, by = 1))
```
I find the degree of polynomial with the minimum error:
```{r, include = TRUE}
sprintf("Polynomial Degree with smallest Test MSE: %.0f",
        which.min(LOOCV))
```
The $\textbf{Polynmoial Degree 6}$ model is chosen.

## 1b. Compute 10-fold CV
I do the same thing, but set $K = 10$:
```{r, include = TRUE}
set.seed(15)
TCV <- c()
for (i in 1:10){
  polyTCV <- glm(sprintf('medv ~ poly(lstat, %f)', i), data = Boston)
  TCV <- append(TCV,boot::cv.glm(data = Boston,glmfit = polyTCV,
                                   K = 10)$delta[2])
}
TCVFrame <- data.frame(cbind(1:10, TCV))
colnames(TCVFrame) <- c("Degree", "TenFoldCV")
TCVFrame
```

I graph the TCV plot:
```{r, include = TRUE}
ggplot(data = TCVFrame) + geom_line(aes(x = Degree,y = TenFoldCV)) +
  geom_point(aes(x = Degree, y = TenFoldCV), color = 'red') + 
  scale_x_continuous(breaks = seq(1,10, by = 1))
```
I find the degree of polynomial with the minimum error:
```{r, include = TRUE}
sprintf("Polynomial Degree with smallest Test MSE: %.0f",
        which.min(TCV))
```
The $\textbf{Polynmoial Degree 6}$ model is chosen.

## 1c. Comparison of 10-fold CV, LOOCV
I graph both MSE lines atop each other:
```{r, include = TRUE}
cFrame <- LCVFrame %>% left_join(TCVFrame, by=c('Degree'))
ggplot(data = cFrame) + 
  geom_line(aes(x = Degree,y = TenFoldCV), color = 'blue') +
  geom_point(aes(x = Degree, y = TenFoldCV), color = 'blue') +
  geom_line(aes(x = Degree,y = LOOCV), color = 'red') +
  geom_point(aes(x = Degree, y = LOOCV), color = 'red') + 
  scale_x_continuous(breaks = seq(1,10, by = 1)) + 
  labs(y = "CV MSE")
```

The two methods for MSE follow very similar patterns, with the 10-Fold CV having a slightly lower MSE for most polynomial degrees. This is due to LOOCV having slightly higher variance due to the potential for overfitting caused by using all but one observation in the training data set.


\newpage

#2. Shrinkage Method - Ridge Regression

## 2a. Finding Optimal tuning parameter, Each case
I find the optimal tuning parameter for each of the different proportions for the training set:
```{r, include = TRUE}
set.seed(1)
train_p <- seq(.5, 1, by = .1)

#get all non-NA salary data
nonNAIndices <- which(!is.na(Hitters$Salary))
data_Hit <- Hitters[nonNAIndices,]

#find optimal lambda for each training proportion
for (prop in train_p){
}
summary(lm("Salary ~.", data = data_Hit))
```

\newpage

#3. Shrinkage Method

