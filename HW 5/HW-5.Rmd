---
title: "Homework 5 - Predictive Modeling in Finance and Insurance"
author: "Dennis Goldenberg"
date: "2024-02-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE}
library(MASS)
library(ggplot2)
library(ISLR)
library(glmnet)
library(dplyr)
```

#1. Cross Validation

## 1a. Compute LOOCV
I use the method prescribed in the homework, noting that the default for the cross validation method is Leave-One-Out Cross validation and show the result:
```{r, include = TRUE}
LOOCV <- c()
for (i in 1:10){
  polyLCV <- glm(sprintf('medv ~ poly(lstat, %f)', i), data = Boston)
  LOOCV <- append(LOOCV,boot::cv.glm(data = Boston,glmfit = polyLCV)$delta[2])
}
LCVFrame <- data.frame(cbind(1:10, LOOCV))
colnames(LCVFrame) <- c("Degree", "LOOCV")
LCVFrame
```

I graph the LOOCV plot:
```{r, include = TRUE}
ggplot(data = LCVFrame) + geom_line(aes(x = Degree,y = LOOCV)) +
  geom_point(aes(x = Degree, y = LOOCV), color = 'red') + 
  scale_x_continuous(breaks = seq(1,10, by = 1))
```
I find the degree of polynomial with the minimum error:
```{r, include = TRUE}
sprintf("Polynomial Degree with smallest Test MSE: %.0f",
        which.min(LOOCV))
```
The $\textbf{Polynmoial Degree 6}$ model is chosen.

## 1b. Compute 10-fold CV
I do the same thing, but set $K = 10$:
```{r, include = TRUE}
set.seed(15)
TCV <- c()
for (i in 1:10){
  polyTCV <- glm(sprintf('medv ~ poly(lstat, %f)', i), data = Boston)
  TCV <- append(TCV,boot::cv.glm(data = Boston,glmfit = polyTCV,
                                   K = 10)$delta[2])
}
TCVFrame <- data.frame(cbind(1:10, TCV))
colnames(TCVFrame) <- c("Degree", "TenFoldCV")
TCVFrame
```

I graph the TCV plot:
```{r, include = TRUE}
ggplot(data = TCVFrame) + geom_line(aes(x = Degree,y = TenFoldCV)) +
  geom_point(aes(x = Degree, y = TenFoldCV), color = 'red') + 
  scale_x_continuous(breaks = seq(1,10, by = 1))
```
I find the degree of polynomial with the minimum error:
```{r, include = TRUE}
sprintf("Polynomial Degree with smallest Test MSE: %.0f",
        which.min(TCV))
```
The $\textbf{Polynmoial Degree 6}$ model is chosen.

## 1c. Comparison of 10-fold CV, LOOCV
I graph both MSE lines atop each other:
```{r, include = TRUE}
cFrame <- left_join(LCVFrame, TCVFrame, by=c('Degree'))
ggplot(data = cFrame) + 
  geom_line(aes(x = Degree,y = TenFoldCV), color = 'blue') +
  geom_point(aes(x = Degree, y = TenFoldCV), color = 'blue') +
  geom_line(aes(x = Degree,y = LOOCV), color = 'red') +
  geom_point(aes(x = Degree, y = LOOCV), color = 'red') + 
  scale_x_continuous(breaks = seq(1,10, by = 1)) + 
  labs(y = "CV MSE")
```

The two methods for MSE follow very similar patterns, with the 10-Fold CV having a slightly lower MSE for most polynomial degrees. This is due to LOOCV having slightly higher variance due to the potential for overfitting caused by using all but one observation in the training data set.


\newpage

#2. Shrinkage Method - Ridge Regression

## 2a. Varying Training Set Size
I find the optimal tuning parameter and minimum MSE for each of the different proportions for the training set:
```{r, include = TRUE}
set.seed(1)
train_p <- seq(.5, 1, by = .1)

#get all non-NA salary data
nonNAIndices <- which(!is.na(Hitters$Salary))
data_Hit <- Hitters[nonNAIndices,]
x <- model.matrix(Salary ~., data = data_Hit)
y <- data_Hit$Salary
lambdas <- c()
MSE <- c()

#find optimal lambda, MSE for for each training proportion
for (prop in train_p){
  train_samp <- sample(1:length(nonNAIndices), 
                       size = as.integer(prop * length(rownames(data_Hit))),
                       replace = F)
  test_samp <- setdiff(1:length(nonNAIndices), train_samp)
  optMod <- cv.glmnet(x[train_samp,],y[train_samp],type.measure =
                      "mse",alpha = 0,family="gaussian")
  lambdas <- append(lambdas, optMod$lambda.1se)
  if (prop < 1){
  predictions <- predict(optMod, newx = x[test_samp,], s = optMod$lambda.1se)
  MSE <- append(MSE,sum((predictions - y[test_samp])^2)/(length(y[test_samp])))
  } else {
    MSE <- append(MSE, 0)
  }
}
```

I plot lambdas vs. proportion of dataset:
```{r, include = TRUE}
lFrame <- cbind.data.frame(round(train_p * 100, 0), lambdas)
names(lFrame) <- c("% of data in Train", "Optimal Lambda")
ggplot(data = lFrame) +
  geom_line(aes(x = `% of data in Train`, y = `Optimal Lambda`)) + 
  geom_point(aes(x = `% of data in Train`, y = `Optimal Lambda`), color='red') + 
  labs(title = "Optimal Tuning Parameter vs. Proportion of Data for Train") + 
  theme(plot.title = element_text(hjust = 0.5))
```

Then, I plot MSE vs. proportion of dataset:
```{r, include = TRUE}
lFrameMSE <- cbind.data.frame(round(train_p * 100, 0), MSE)
names(lFrameMSE) <- c("% of data in Train", "MSE")
ggplot(data = lFrameMSE) +
  geom_line(aes(x = `% of data in Train`, y = MSE)) + 
  geom_point(aes(x = `% of data in Train`, y = MSE), color='red') + 
  labs(title = "Mean Squared Error vs. Proportion of Data for Train") + 
  theme(plot.title = element_text(hjust = 0.5))
```
It seems as though using $\textbf{70\%}$ of the data seems to minimize test Mean Squared error (100% train data cannot be considered, as no test MSE can be calculated when there is no test data).

## 2b. Varying Random Seed
I run essentially the same method as above, but this time I vary the seed:
```{r, include = TRUE}
lambdas2 <- c()
for (j in 1:10){
  set.seed(j)
  train_samp2 <- sample(1:length(nonNAIndices), 
                       size = as.integer(0.85 * length(rownames(data_Hit))),
                       replace = F)
  set.seed(1)
  optMod2 <- cv.glmnet(x[train_samp2,],y[train_samp2],type.measure =
                      "mse",alpha = 0,family="gaussian")
  lambdas2 <- append(lambdas2, optMod2$lambda.1se)
}
```

Now I plot the optimal lambda value against the seed values:
```{r, include = TRUE}
lFrame2 <- cbind.data.frame(1:10, lambdas2)
names(lFrame2) <- c("Seed", "Optimal Lambda")
ggplot(data = lFrame2) +
  geom_line(aes(x = Seed, y = `Optimal Lambda`)) + 
  geom_point(aes(x = Seed, y = `Optimal Lambda`), color='red') + 
  scale_x_continuous(breaks = seq(1,10, by = 1)) + 
  labs(title = "Optimal Tuning Parameter vs. Random Seed") + 
  theme(plot.title = element_text(hjust = 0.5))
```

## 2c. Takeaways, Observations From Above
From the graphs plotting optimal lambdas against training proportion and seed, it seems as though proportion of training data has a noticeable impact on optimal lambda, while seed does not. As the proportion of data being trained on gets higher, there seems to be a downward trend in optimal lambda; a reason for this is that there is more data to glean which variables are truly important and which aren't, so higher betas on the important variables decrease the error function by enough to compensate for the penalty. However, the fact that there is no significant trend with respect to the seed may be the result of the fact that 10-fold cross validation is being used; there is significant overlap between any 2 randomly generated training sets that cover 90% of the data, so the Cross Validation error calculations for each seed are likely very similar, as nothing fundamentally changes from seed to seed.
\newpage

#3. Shrinkage Method

